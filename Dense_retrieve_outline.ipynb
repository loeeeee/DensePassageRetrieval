{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Llq6FaDj5ayV",
        "outputId": "ec15cdf6-5c54-4eb6-d386-5e32290c4f84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWTMDB2v5qD-",
        "outputId": "3aefa308-f168-4f5b-b66c-9f457cab846c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV0AzbsX56Xz",
        "outputId": "c984bb90-4665-4847-f7ff-519dc24e3041"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_corpus_data():\n",
        "  return [\n",
        "      \"Usually the people called Daniel are bad.\",\n",
        "      \"Cosine similarity is a measure of similarity, often used to measure document similarity in text analysis.\",\n",
        "      \"Peter likes fish, steak, and roast chicken\"\n",
        "  ]\n",
        "\n",
        "corpus = load_corpus_data()"
      ],
      "metadata": {
        "id": "0URbvB-I6M0e"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_search_queries():\n",
        "  return [\n",
        "      \"which name is bad\",\n",
        "      \"what is cosine similarity\",\n",
        "      \"who likes to eat meat\"\n",
        "  ]\n",
        "\n",
        "queries = load_search_queries()"
      ],
      "metadata": {
        "id": "_JcwfVNF8f4W"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BQtG_SDSRoaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "corpus_embeddings = []\n",
        "for text in corpus:\n",
        "    input_ids = torch.tensor(tokenizer.encode(text, add_special_tokens=True)).unsqueeze(0)\n",
        "    outputs = model(input_ids)\n",
        "    last_hidden_states = outputs[0]\n",
        "    avg_pooling = torch.mean(last_hidden_states, dim=1).squeeze()\n",
        "    corpus_embeddings.append(avg_pooling.detach().numpy())\n",
        "\n"
      ],
      "metadata": {
        "id": "c31ygHFa6f0r"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_embeddings = []\n",
        "for text in queries:\n",
        "    input_ids = torch.tensor(tokenizer.encode(text, add_special_tokens=True)).unsqueeze(0)\n",
        "    outputs = model(input_ids)\n",
        "    last_hidden_states = outputs[0]\n",
        "    avg_pooling = torch.mean(last_hidden_states, dim=1).squeeze()\n",
        "    query_embeddings.append(avg_pooling.detach().numpy())\n",
        "\n",
        "    # # Tokenize the input text and get the input IDs and attention mask\n",
        "    # encoded_input = tokenizer(text, return_tensors='pt')\n",
        "    # input_ids = encoded_input['input_ids']\n",
        "    # attention_mask = encoded_input['attention_mask']\n",
        "\n",
        "    # # Pass the input through the BERT model to get the last_hidden_states tensor\n",
        "    # last_hidden_states = model(input_ids, attention_mask)[0]\n",
        "\n",
        "    # # Get the attention weights from the last layer of the BERT model\n",
        "    # attention_layer = model.encoder.layer[-1].attention.self\n",
        "    # query_weight = attention_layer.query.weight\n",
        "    # attention_scores = torch.matmul(last_hidden_states, query_weight.transpose(0, 1))\n",
        "    # attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
        "\n",
        "    # # Compute a weighted sum of the embeddings using the attention weights\n",
        "    # sentence_embedding = torch.matmul(attention_weights.transpose(1, 2), last_hidden_states * attention_mask.unsqueeze(-1)).sum(dim=1) / attention_mask.sum(dim=1, keepdim=True)\n",
        "    # query_embeddings.append(sentence_embedding.squeeze().detach().numpy())\n",
        "    "
      ],
      "metadata": {
        "id": "rsRD6oOs8ed_"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "for query_embedding in query_embeddings:\n",
        "  relevance_scores = cosine_similarity(query_embedding.reshape(1, -1), corpus_embeddings)\n",
        "  sorted_indices = relevance_scores[0].argsort()[::-1]\n",
        "  relevant_documents = [corpus[i] for i in sorted_indices[:10]]\n",
        "  print(relevance_scores)\n",
        "  print(relevant_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCcm2_vj84YU",
        "outputId": "7fd0eb59-8569-46ca-94fe-fdf77d7f8180"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.61023784 0.4218006  0.46227834]]\n",
            "['Usually the people called Daniel are bad.', 'Peter likes fish, steak, and roast chicken', 'Cosine similarity is a measure of similarity, often used to measure document similarity in text analysis.']\n",
            "[[0.4857527  0.60021305 0.44286966]]\n",
            "['Cosine similarity is a measure of similarity, often used to measure document similarity in text analysis.', 'Usually the people called Daniel are bad.', 'Peter likes fish, steak, and roast chicken']\n",
            "[[0.5862737 0.4151404 0.6338297]]\n",
            "['Peter likes fish, steak, and roast chicken', 'Usually the people called Daniel are bad.', 'Cosine similarity is a measure of similarity, often used to measure document similarity in text analysis.']\n"
          ]
        }
      ]
    }
  ]
}